{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of neural networks\n",
    "\n",
    "To train neural networks, we first need to load the dataset. If you don't know how `AirfoilDataset` and `FileDataFiles` works, please refer to  `process_dataset.ipynb`.\n",
    "The following codes showcase how to load a dataset that is used in the single-parameter test in our manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 125/125 [00:00<00:00, 459.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from airfoil_diffusion.airfoil_datasets import *\n",
    "from airfoil_diffusion.networks import *\n",
    "from airfoil_diffusion.trainer import *\n",
    "\n",
    "if not os.path.exists(\"./datasets/1_parameter/data/\"):\n",
    "    files=[file for file in os.listdir(\"./datasets/1_parameter/\") if file.endswith(\".zip\")]\n",
    "    for file in tqdm(files): \n",
    "        f=zipfile.ZipFile(\"./datasets/1_parameter/\"+file,'r')\n",
    "        for file in f.namelist():\n",
    "            f.extract(file,\"./datasets/1_parameter/data/\")\n",
    "        f.close() \n",
    "\n",
    "train_dataset = AirfoilDataset(FileDataFiles(\"./datasets/1_parameter/train_cases.txt\",base_path=\"./datasets/1_parameter/data/\"),\n",
    "                               data_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can import our network, `AifNet`. You can use `show_config_options` function to list all the possible configurations of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandatory Configuration:\n",
      "    dim_in (int): The input dim of the model.\n",
      "    dim_out (int): The output dim of the model.\n",
      "    dim_basic (int): The basic dimensions in each layer. The real dimension numbers are dim_basic$\\times$dim_multipliers.\n",
      "    dim_multipliers (list): A list used to control the depth and the size of the net. There will be len(dim_multipliers)-1 down/up blocks in the net. The number of input/output channels of each block will also be determined by the elements of this list(dim_basic$\\times$dim_multipliers). For instance, if the dim_multipliers is [1 2 4 8]. There will be 3 down/up blocks. The input/output channel of these blocks are (dim_basic, 2$\\times$dim_basic), (2$\\times$dim_basic, 4$\\times$dim_basic) and (4$\\times$dim_basic, 8$\\times$dim_basic). The size of neckblock will be  8$\\times$dim_basic $\\times$ input_channel/$2^3$ $\\times$ input_channel/$2^3$. If the first elements is 0, the input channel of the first down layer will be the dim_in and the output channel of the last down layer will be dim_out.\n",
      "\n",
      "Optional Configuration:\n",
      "    attention_layers (list, default value: [3, 4]): The layers where attention blocks are added.\n",
      "    condition_layers (list, default value: [-2]): The layers where condition are added using cross attention. '-2' means that we won't use cross attention to add the condition.\n",
      "    use_input_condition (bool, default value: True): Whether to add the condition into input channels;.\n",
      "    skip_connection_scale (float, default value: 1.0): The scale of the skip connection. The output of each down block will be multiplied by this value before being added to the input of the corresponding up block.\n",
      "    depth_each_layer (int, default value: 2): The depth of each layer.\n",
      "    dim_encoded_time (int, default value: 8): The dimension of the time embeddings.\n",
      "    dim_condition (int, default value: 3): The dimension of the condition.\n",
      "    heads_attention (int, default value: 4): The number of heads in the attention blocks.\n",
      "    linear_attention (bool, default value: False): Whether to use linear attention.\n",
      "    condition_dim (int, default value: 0): The dimensions of conditions. Please set this value as 0 when no condition is provided.\n"
     ]
    }
   ],
   "source": [
    "AifNet().show_config_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the configurations can be specified either by a YAML configuration file or by directly inputting when creating the network. If you give both the YAML file and parameters during the initialization, the latter will overwrite the configuration item provided by the YAML function.\n",
    "You can use `show_current_configs` function to show the current configuration of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_layers: [2, 3]\n",
      "condition_layers: [-2]\n",
      "depth_each_layer: 2\n",
      "dim_basic: 16\n",
      "dim_condition: 3\n",
      "dim_encoded_time: 8\n",
      "dim_in: 6\n",
      "dim_multipliers: [1, 2, 4, 4]\n",
      "dim_out: 3\n",
      "heads_attention: 4\n",
      "linear_attention: False\n",
      "skip_connection_scale: 0.707\n",
      "use_input_condition: True\n",
      "condition_dim: 0\n"
     ]
    }
   ],
   "source": [
    "network = AifNet(\"./pre_trained/single_parameter/32/diffusion/network_configs.yaml\")\n",
    "#network = AifNet(\"./pre_trained/single_parameter/32/diffusion/network_configs.yaml\",condition_layers=[-3])\n",
    "network.show_current_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide several trainer classes for training procedures that can be easily invoked. You can also use `show_config_options` function to see all the possible configurations of the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mandatory Configuration:\n",
      "    name (str): Name of the training.\n",
      "    save_path (str): Path to save the training results.\n",
      "    batch_size_train (int): Batch size for training.\n",
      "    epochs (int): Number of epochs for training.\n",
      "    lr (float): Initial learning rate.\n",
      "\n",
      "Optional Configuration:\n",
      "    device (str, default value: cpu): Device for training.\n",
      "    random_seed (int): Random seed for training. Default is the same as batch_size_train.\n",
      "    batch_size_validation (int): Batch size for validation. Default is the same as batch_size_train.\n",
      "    shuffle_train (bool, default value: True): Whether to shuffle the training dataset.\n",
      "    shuffle_validation (bool): Whether to shuffle the validation dataset. Default is the same as shuffle_train.\n",
      "    num_workers_train (int, default value: 0): Number of workers for training.\n",
      "    num_workers_validation (int): Number of workers for validation. Default is the same as num_workers_train.\n",
      "    validation_epoch_frequency (int, default value: 1): Frequency of validation.\n",
      "    optimizer (str, possible option: ['AdamW', 'Adam', 'SGD'], default value: AdamW): Optimizer for training.\n",
      "    lr_scheduler (str, default value: cosine): Learning rate scheduler for training\n",
      "    final_lr (float): Final learning rate for lr_scheduler.\n",
      "    warmup_epoch (int, default value: 0): Number of epochs for learning rate warm up.\n",
      "    record_iteration_loss (bool, default value: False): Whether to record iteration loss.\n",
      "    save_epoch (int): Frequency of saving checkpoints.\n",
      "    diffusion_step (int, default value: 200): The number of diffusion steps.\n"
     ]
    }
   ],
   "source": [
    "diffusion_trainer=DiffusionTrainer()\n",
    "diffusion_trainer.show_config_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `train_from_scratch` function to train a network from scratch or `train_from_checkpoint` function to train from a checkpoint. Similarly, you can also give a YAML file or directly set the configurations in the function.\n",
    "\n",
    "Our trainer provides some useful features like checkpoint saving, loss recorder with TensorBoard, and configuration saving. You can also utilize the `TrainedProject` class we provided to manage the trained project. Details can be found in airfoil_diffusion/trainer_base.py and airfoil_diffusion/trainer.py. Explore by yourself and enjoy!\n",
    "\n",
    "Now, let's train our networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the diffusion model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_trainer.train_from_scratch(name=\"diffusion\",\n",
    "                                     network=network,\n",
    "                                     train_dataset=train_dataset,\n",
    "                                     path_config_file=\"./pre_trained/train_configs.yaml\",\n",
    "                                     save_path=\"./training/single_parameter/32/\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train heteroscedastic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heteroscedastic_trainer=HeteroscedasticTrainer()\n",
    "network = AifNet(\"./pre_trained/single_parameter/32/heteroscedastic/network_configs.yaml\")\n",
    "heteroscedastic_trainer.train_from_scratch(name=\"heteroscedastic\",\n",
    "                                            network=network,\n",
    "                                            train_dataset=train_dataset,\n",
    "                                            path_config_file=\"./pre_trained/train_configs.yaml\",\n",
    "                                            save_path=\"./training/single_parameter/32/\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train BNNs with different KL scaling:\n",
    "\n",
    "Note that you need to install baysian_torch (https://github.com/IntelLabs/bayesian-torch) first. We used version 0.3.0 for examples here and cases in the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian_torch.models.dnn_to_bnn import dnn_to_bnn\n",
    "bnn_trainer=BNNTrainer()\n",
    "const_bnn_prior_parameters = read_configs(\"./pre_trained/single_parameter/32/BNN/bnn_configs.yaml\")\n",
    "\n",
    "for KL_scaling in [0.0001,0.001,0.01]:\n",
    "    network = AifNet(\"./pre_trained/single_parameter/32/BNN/network_configs.yaml\")\n",
    "    dnn_to_bnn(network, const_bnn_prior_parameters)\n",
    "    bnn_trainer.train_from_scratch(name=\"BNN_{}\".format(KL_scaling),\n",
    "                                    network=network,\n",
    "                                    train_dataset=train_dataset,\n",
    "                                    path_config_file=\"./pre_trained/train_configs.yaml\",\n",
    "                                    save_path=\"./training/single_parameter/32/\",\n",
    "                                    KL_scaling=KL_scaling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ef7c4d1f3df8db587059df561e939e373fe7e11eb2e3f25a593801b878b36bde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
